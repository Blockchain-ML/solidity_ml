{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID       v1       v2       v3       v4       v5       v6       v7  \\\n",
      "0      10001  0.41463  1.18500  1.12590 -0.63784 -1.60940 -0.19869 -0.23010   \n",
      "1      10002 -1.68950 -0.63024 -0.23417 -2.21810 -0.36157 -0.52164 -0.31883   \n",
      "2      10003 -1.71670  2.40120  0.44570  1.14400 -0.53560 -0.69040  0.39990   \n",
      "3      10004 -0.07260  0.31790 -1.38690  1.83000 -1.08930 -1.15900  0.05750   \n",
      "4      10005  0.32088 -1.08450 -1.15780 -0.63845  1.03060  0.38707  0.95113   \n",
      "...      ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "62178  96207 -1.45150 -0.95600 -0.89920 -0.80340 -0.83730  2.07520 -2.71140   \n",
      "62179  96208  1.27570 -1.32360  2.00620 -1.61870 -0.72743  0.58177 -2.55890   \n",
      "62180  96209  0.35809 -2.22570  0.22366 -2.89240  0.43210 -1.64220 -0.12659   \n",
      "62181  96210  0.40804 -2.27950 -0.83962 -1.03440  1.04950  1.32130  1.27670   \n",
      "62182  96211 -0.74982  0.35505 -0.47464 -2.38330 -1.45550 -0.62356 -1.79640   \n",
      "\n",
      "            v8       v9  ...      v491     v492      v493      v494     v495  \\\n",
      "0     -0.69190 -0.99791  ...  0.038877 -0.81235  0.933330  0.534380  0.75724   \n",
      "1      1.06740  0.75016  ... -1.366200 -0.50756  0.014574  1.254400  1.47300   \n",
      "2      1.63330 -0.43470  ...  0.102600  2.38170  1.318500 -0.866900  0.97240   \n",
      "3      0.72450 -0.47780  ...  0.055500  0.48700  1.390100  1.404000 -2.53400   \n",
      "4      0.30132 -1.08750  ...  0.241960  1.93180 -0.930670  0.757390  0.65279   \n",
      "...        ...      ...  ...       ...      ...       ...       ...      ...   \n",
      "62178  1.63910  0.79040  ... -0.935500 -1.42240 -0.239600 -0.080800  0.35160   \n",
      "62179  0.46077 -2.32850  ... -0.523610  0.74670  0.517040 -0.189150  2.19590   \n",
      "62180 -0.65374 -4.12450  ...  1.970200  3.30630  2.595300 -0.936870 -0.88340   \n",
      "62181  0.71817  1.09210  ... -2.847900  0.23018 -0.046523 -0.976680  0.41813   \n",
      "62182 -2.61440 -0.72109  ... -0.410040  1.77290 -2.027400 -0.076067 -3.03920   \n",
      "\n",
      "          v496      v497     v498     v499         Error_Label  \n",
      "0     -1.03830 -0.758880  0.29875 -1.12810  000000000000000100  \n",
      "1      0.45056 -2.005600  1.19390 -2.21030  000000000000000000  \n",
      "2      0.87710 -1.038700  0.78180  0.56360  000000000000000000  \n",
      "3     -1.77610 -1.188000  1.13680 -0.11580  000000000000000000  \n",
      "4     -0.52051  0.406710  0.36767  1.08240  000000000001000000  \n",
      "...        ...       ...      ...      ...                 ...  \n",
      "62178  0.04230  0.039200 -0.14330 -1.16540  000001000011010000  \n",
      "62179 -0.38150  0.150240 -1.41880 -1.57960  010001000000001000  \n",
      "62180  0.23934 -1.231100 -0.55202 -0.23169  000001000000000000  \n",
      "62181 -0.31383 -1.897800 -0.23670 -0.44662  000001000011010000  \n",
      "62182 -1.61440 -0.097925 -1.14480 -0.12252  000001000011010000  \n",
      "\n",
      "[62183 rows x 501 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from pylab import rcParams\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "data = pd.read_csv(r'/home/supriya/pathcontext/vectorization_work/CDS_vectorization_v1.csv',dtype={'Error_Label':object} )\n",
    "print(data)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62183, 501)\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "62178    0\n",
      "62179    0\n",
      "62180    0\n",
      "62181    0\n",
      "62182    0\n",
      "Name: l1, Length: 62183, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# split labels and return new dataframe\n",
    "#Create independent and Dependent Features\n",
    "#data=split_label(data)\n",
    "def split_label_dict(df):\n",
    "    # Change 'label' to whatever naming used in the original DataFrame.\n",
    "    temp_df = df['Error_Label'].str.split(\"\", n = -1, expand = True)\n",
    "    for i in range(1, 19):\n",
    "        df[f'l{1}'] = temp_df[1]\n",
    "        df[f'l{1}'] = pd.to_numeric(df[f'l{1}'])\n",
    "    return df\n",
    "\n",
    "newdata=split_label_dict(data)\n",
    "#print(newdata)\n",
    "#Create independent and Dependent Features\n",
    "columns = newdata.columns.tolist()\n",
    "# Filter the columns to remove data we do not want \n",
    "columns = [c for c in columns if c not in [\"l1\"]]\n",
    "# Store the variable we are predicting \n",
    "target = \"l1\"\n",
    "# Define a random state \n",
    "state = np.random.RandomState(69)\n",
    "X = newdata[columns]\n",
    "Y = newdata[target]\n",
    "# Print the shapes of X & Y\n",
    "print(X.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test, y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=69)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smk = SMOTETomek()\n",
    "X_res,y_res=smk.fit_sample(X_train.astype('float'),y_train)\n",
    "print(X_res.shape,y_res.shape)\n",
    "#print(X_res)\n",
    "#print(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      0    1\n",
       "l1               \n",
       "0      17676    0\n",
       "1          0  979"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hyperparameter optimization using RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import xgboost\n",
    "\n",
    "classifier=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.5, gamma=0.4, learning_rate=0.1,\n",
    "       max_delta_step=0, max_depth=6, min_child_weight=7, missing=None,\n",
    "       n_estimators=100, n_jobs=1, nthread=None,\n",
    "       objective='binary:logistic', random_state=69, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       subsample=1)\n",
    "model=classifier.fit(X_res, y_res)\n",
    "# Predicting the test set results\n",
    "y_predict = classifier.predict(X_test)\n",
    "print(accuracy_score(y_test,y_predict))\n",
    "pd.crosstab(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save History as Pickle\n",
    "\n",
    "filename = '/home/supriya/pathcontext/vectorization_work/xgboost/xgboost_SMOTETomek/l1xgboost_smote.pickle'\n",
    "with open(filename, 'wb') as fh:\n",
    "    pickle.dump(model, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "results=classification_report(y_test,y_predict)\n",
    "accuracy_score=accuracy_score(y_test,y_predict)\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "roc=roc_auc_score(y_test,y_predict)\n",
    "print(\"ROC_ACCuracy:\",roc)\n",
    "print(cm)\n",
    "print(results) \n",
    "#print(accuracy_score)\n",
    "\n",
    "# Save Results as Pickle\n",
    "filename = '/home/supriya/pathcontext/vectorization_work/xgboost/xgboost_SMOTETomek/l1xgboost_smote.txt'\n",
    "with open(filename, 'a') as fh:\n",
    "    fh.write(str(results)+\"\\n\" +\"ROC_ACCuracy:  \"+str(roc) +\"\\n\"+\"Confusion_Mtraix\"+\"\\n\"+str(cm)+\"\\n\"+\"accuracy_score: \"+str(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
