{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from pylab import rcParams\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "# Set the ERROR Label Number to establish the model on that specific ERROR Label\n",
    "for ERROR_LABEL in range(1, 10):\n",
    "\n",
    "    #data = pd.read_csv(r'/home/supriya/CDS_vectorization_v2x.csv',dtype={'Error_Label':object} )\n",
    "    data = pd.read_csv(r'/home/supriya/Hybrid_v2_v2x.csv',dtype={'Error_Label':object} )\n",
    "\n",
    "    print(data.shape)\n",
    "    #data.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # split labels and return new dataframe\n",
    "    #Create independent and Dependent Features\n",
    "\n",
    "\n",
    "\n",
    "    def split_label_dict(df):\n",
    "        # Change 'label' to whatever naming used in the original DataFrame.\n",
    "        temp_df = df['Error_Label'].str.split(\"\", n = -1, expand = True)\n",
    "        for i in range(1, 19):\n",
    "            df[f'l{i}'] = temp_df[i]\n",
    "            df[f'l{i}'] = pd.to_numeric(df[f'l{i}'])\n",
    "        return df\n",
    "\n",
    "    newdata=split_label_dict(data)\n",
    "    #print(newdata)\n",
    "    #Create independent and Dependent Features\n",
    "    df = pd.DataFrame(data)\n",
    "    new_df = df.drop(['Error_Label','l18','l17','l16','l15','l14','l13','l12','l11','l10','l9','l8','l7','l6','l5','l3','l4','l2','l1', 'ID'],axis=1)\n",
    "\n",
    "    X = new_df\n",
    "    # Store the variable we are predicting \n",
    "    Y = newdata[f\"l{ERROR_LABEL}\"]\n",
    "\n",
    "    # Print the shapes of X & Y\n",
    "    print(X.shape,Y.shape)\n",
    "\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train,X_test, y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=42,stratify=Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Feature Scaling\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc_X = StandardScaler()\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.datasets import make_classification\n",
    "    from imblearn.over_sampling import SVMSMOTE\n",
    "    from imblearn.combine import SMOTEENN\n",
    "    #smk=SVMSMOTE()\n",
    "    #smk = SMOTETomek()\n",
    "    smk=SMOTEENN()\n",
    "    X_res,y_res=smk.fit_sample(X_train.astype('float'),y_train)\n",
    "    print(X_res.shape,y_res.shape)\n",
    "    #print(X_res)\n",
    "    #print(y_res)\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    ## Hyperparameter optimization using RandomizedSearchCV\n",
    "    from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "    import xgboost\n",
    "\n",
    "    classifier= xgboost.XGBClassifier(eta=0.8,min_child_weight=5,max_depth=50,max_delta_step=8,reg_alpha=2,grow_policy='lossguide',subsample=0.6,tree_method='approx',max_leaves=1)\n",
    "    classifier.fit(X_res, y_res)\n",
    "    # Predicting the test set results\n",
    "    y_predict = classifier.predict(X_test)\n",
    "    print(accuracy_score(y_test,y_predict))\n",
    "    pd.crosstab(y_test,y_predict)\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    print(classification_report(y_test,y_predict))\n",
    "    print(accuracy_score(y_test,y_predict))\n",
    "\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "    from sklearn.model_selection import cross_val_score \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import pickle\n",
    "    results=classification_report(y_test,y_predict)\n",
    "    accuracy_score=accuracy_score(y_test,y_predict)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    roc=roc_auc_score(y_test,y_predict)\n",
    "\n",
    "    print(cm)\n",
    "    print(results) \n",
    "    #print(accuracy_score)\n",
    "\n",
    "    # Save Results as txt\n",
    "    filename = f'/home/supriya/pathcontext/vectorization_work/xgboost/SMOTEENN/l{ERROR_LABEL}comb_SMOTEENN_results.txt'\n",
    "    with open(filename, 'a') as fh:\n",
    "        fh.write(str(results)+\"\\n\" +\"ROC_ACCuracy:  \"+str(roc) +\"\\n\"+\"Confusion_Matrix\"+\"\\n\"+str(cm)+\"\\n\"+\"accuracy_score: \"+str(accuracy_score))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from pylab import rcParams\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "for ERROR_LABEL in range(1, 10):\n",
    "    #data = pd.read_csv(r'/home/supriya/CDS_vectorization_v2x.csv',dtype={'Error_Label':object} )\n",
    "    data = pd.read_csv(r'/home/supriya/Hybrid_v2_v2x.csv',dtype={'Error_Label':object} )\n",
    "    print(data.shape)\n",
    "    #data.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # split labels and return new dataframe\n",
    "    #Create independent and Dependent Features\n",
    "\n",
    "    def split_label_dict(df):\n",
    "        # Change 'label' to whatever naming used in the original DataFrame.\n",
    "        temp_df = df['Error_Label'].str.split(\"\", n = -1, expand = True)\n",
    "        for i in range(1, 19):\n",
    "            df[f'l{i}'] = temp_df[i]\n",
    "            df[f'l{i}'] = pd.to_numeric(df[f'l{i}'])\n",
    "        return df\n",
    "\n",
    "    newdata=split_label_dict(data)\n",
    "    #print(newdata)\n",
    "    #Create independent and Dependent Features\n",
    "    df = pd.DataFrame(data)\n",
    "    new_df = df.drop(['Error_Label','l18','l17','l16','l15','l14','l13','l12','l11','l10','l9','l8','l7','l6','l5','l3','l4','l2','l1', 'ID'],axis=1)\n",
    "    #new_df = data.drop(['Error_Label','ID'],axis=1)\n",
    "    X = new_df\n",
    "    # Store the variable we are predicting \n",
    "    Y = newdata[f\"l{ERROR_LABEL}\"]\n",
    "\n",
    "    # Print the shapes of X & Y\n",
    "    print(X.shape,Y.shape)\n",
    "\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train,X_test, y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=42,stratify=Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Feature Scaling\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc_X = StandardScaler()\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.datasets import make_classification\n",
    "    from imblearn.over_sampling import SVMSMOTE\n",
    "    smk = SVMSMOTE()\n",
    "    #smk = SMOTE()\n",
    "    #smk = SMOTETomek()\n",
    "    X_res,y_res=smk.fit_sample(X_train.astype('float'),y_train)\n",
    "    print(X_res.shape,y_res.shape)\n",
    "    #print(X_res)\n",
    "    #print(y_res)\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    ## Hyperparameter optimization using RandomizedSearchCV\n",
    "    from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "    #import xgboost\n",
    "\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "    # Fitting the classifier into the Training set\n",
    "\n",
    "    classifier = AdaBoostClassifier(\n",
    "        DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=200,learning_rate=1\n",
    "    )\n",
    "    classifier.fit(X_res, y_res)\n",
    "    # Predicting the test set results\n",
    "    y_predict = classifier.predict(X_test)\n",
    "    print(accuracy_score(y_test,y_predict))\n",
    "    pd.crosstab(y_test,y_predict)\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    print(classification_report(y_test,y_predict))\n",
    "    print(accuracy_score(y_test,y_predict))\n",
    "\n",
    "    from sklearn.metrics import classification_report,confusion_matrix\n",
    "    from sklearn.model_selection import cross_val_score \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    import pickle\n",
    "    results=classification_report(y_test,y_predict)\n",
    "    accuracy_score=accuracy_score(y_test,y_predict)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    roc=roc_auc_score(y_test,y_predict)\n",
    "\n",
    "    print(cm)\n",
    "    print(results) \n",
    "    #print(accuracy_score)\n",
    "\n",
    "    # Save Results as txt\n",
    "    filename = f'/home/supriya/pathcontext/vectorization_work/AdaBoost/SMOTEENN/l{ERROR_LABEL}comb_SMOTEENN_results.txt'\n",
    "    with open(filename, 'a') as fh:\n",
    "        fh.write(str(results)+\"\\n\" +\"ROC_ACCuracy:  \"+str(roc) +\"\\n\"+\"Confusion_Matrix\"+\"\\n\"+str(cm)+\"\\n\"+\"accuracy_score: \"+str(accuracy_score))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
